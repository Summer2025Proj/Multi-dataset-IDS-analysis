{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4699e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Data ---\n",
      "Searching for CSV files in: ./kaggle/input/ids-intrusion-csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimized IDS Notebook (Enhanced Multiclass Version) for CSE-CIC-IDS2018\n",
    "# This code is designed to be run in Google Colab.\n",
    "\n",
    "# --- Step 0: Initial Setup and Library Imports ---\n",
    "# Make sure you have the necessary libraries installed.\n",
    "# If you run into \"ModuleNotFoundError\", you might need to install them:\n",
    "# !pip install pandas scikit-learn numpy seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings that might clutter output (e.g., DtypeWarning from pandas)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Step 1: Mount Google Drive ---\n",
    "# print(\"Mounting Google Drive...\")\n",
    "# drive.mount('/content/drive')\n",
    "# print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "# --- Step 2: Configuration for CSE-CIC-IDS2018 Dataset ---\n",
    "dataset_path = './kaggle/input/ids-intrusion-csv'\n",
    "label_column_name = 'Label'\n",
    "sample_size = 50000\n",
    "\n",
    "# --- Step 3: Data Loading and Concatenation ---\n",
    "print(f\"\\n--- Loading Data ---\")\n",
    "print(f\"Searching for CSV files in: {dataset_path}\")\n",
    "all_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ffff88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not all_files:\n",
    "    print(f\"ERROR: No CSV files found in {dataset_path}. Please check the path and ensure files exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4cb027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 CSV files. Starting to load them...\n",
      "Successfully loaded: 02-14-2018.csv\n",
      "Successfully loaded: 02-15-2018.csv\n",
      "Successfully loaded: 02-16-2018.csv\n",
      "Successfully loaded: 02-20-2018.csv\n",
      "Successfully loaded: 02-21-2018.csv\n",
      "Successfully loaded: 02-22-2018.csv\n",
      "Successfully loaded: 02-23-2018.csv\n",
      "Successfully loaded: 02-28-2018.csv\n",
      "Successfully loaded: 03-01-2018.csv\n",
      "Successfully loaded: 03-02-2018.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(all_files)} CSV files. Starting to load them...\")\n",
    "df_list = []\n",
    "for f in all_files:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(f, low_memory=False, encoding='utf-8')\n",
    "        df_list.append(df_temp)\n",
    "        print(f\"Successfully loaded: {os.path.basename(f)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Error loading {os.path.basename(f)} with UTF-8: {e}\")\n",
    "        print(f\"Attempting with 'latin1' encoding...\")\n",
    "        try:\n",
    "            df_temp = pd.read_csv(f, low_memory=False, encoding='latin1')\n",
    "            df_list.append(df_temp)\n",
    "            print(f\"Successfully loaded with 'latin1': {os.path.basename(f)}\")\n",
    "        except Exception as e_latin1:\n",
    "            print(f\"ERROR: Failed to load {os.path.basename(f)} with 'latin1' encoding either: {e_latin1}\")\n",
    "            print(\"Skipping this file.\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74d5c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 16.0 MiB for an array with shape (2, 1048575) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_list:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df_combined = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll datasets loaded and concatenated successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal combined shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_combined.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    187\u001b[39m     fastpath = blk.values.dtype == values.dtype\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     values = \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     fastpath = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[39m, in \u001b[36m_concatenate_join_units\u001b[39m\u001b[34m(join_units, copy)\u001b[39m\n\u001b[32m    483\u001b[39m     concat_values = ensure_block_shape(concat_values, \u001b[32m2\u001b[39m)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     concat_values = \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype != empty_dtype_future:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype == concat_values.dtype:\n\u001b[32m    490\u001b[39m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:126\u001b[39m, in \u001b[36mconcat_compat\u001b[39m\u001b[34m(to_concat, axis, ea_compat_axis)\u001b[39m\n\u001b[32m    115\u001b[39m         warnings.warn(\n\u001b[32m    116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe behavior of array concatenation with empty entries is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdeprecated. In a future version, this will no longer exclude \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m             stacklevel=find_stack_level(),\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     to_concat = [\u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m to_concat]\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(to_concat[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# i.e. isinstance(to_concat[0], ExtensionArray)\u001b[39;00m\n\u001b[32m    130\u001b[39m     to_concat_eas = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[ExtensionArray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 16.0 MiB for an array with shape (2, 1048575) and data type object"
     ]
    }
   ],
   "source": [
    "if df_list:\n",
    "    df_combined = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    print(\"\\nAll datasets loaded and concatenated successfully.\")\n",
    "    print(f\"Original combined shape: {df_combined.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the combined dataframe:\")\n",
    "    print(df_combined.head())\n",
    "    print(f\"\\nColumns in the combined dataframe (showing first/last for brevity):\")\n",
    "    print(df_combined.columns.tolist()[:5], \"...\", df_combined.columns.tolist()[-5:])\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No dataframes were successfully loaded. Cannot proceed with analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Data Preprocessing ---\n",
    "print(f\"\\n--- Data Preprocessing ---\")\n",
    "if label_column_name not in df_combined.columns:\n",
    "    print(f\"ERROR: Label column '{label_column_name}' not found. Checking for common alternatives...\")\n",
    "    if 'Attack' in df_combined.columns:\n",
    "        label_column_name = 'Attack'\n",
    "        print(f\"Using 'Attack' as the label column.\")\n",
    "    else:\n",
    "        raise KeyError(f\"Required label column ('{label_column_name}' or 'Attack') not found in the dataset.\")\n",
    "\n",
    "print(f\"Value counts for the original '{label_column_name}' column:\")\n",
    "print(df_combined[label_column_name].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_rows = df_combined.shape[0]\n",
    "df_combined.dropna(subset=[label_column_name], inplace=True)\n",
    "rows_dropped_label_na = initial_rows - df_combined.shape[0]\n",
    "if rows_dropped_label_na > 0:\n",
    "    print(f\"Dropped {rows_dropped_label_na} rows with NA values in the '{label_column_name}' column.\")\n",
    "print(f\"Shape after dropping NA in label column: {df_combined.shape}\")\n",
    "\n",
    "if df_combined.shape[0] > sample_size:\n",
    "    df = df_combined.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Sampled down to {df.shape[0]} rows for faster processing.\")\n",
    "else:\n",
    "    df = df_combined.copy()\n",
    "    print(\"Dataset size is smaller than or equal to sample_size, no sampling performed.\")\n",
    "\n",
    "columns_to_drop_initial = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'SimillarHTTP', 'Unnamed: 0']\n",
    "columns_to_drop_initial = [col for col in columns_to_drop_initial if col in df.columns]\n",
    "df.drop(columns=columns_to_drop_initial, errors='ignore', inplace=True)\n",
    "print(f\"Shape after dropping identifier/non-informative columns: {df.shape}\")\n",
    "\n",
    "X = df.drop(columns=[label_column_name])\n",
    "y = df[label_column_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Converting feature columns to numeric types...\")\n",
    "for col in X.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "print(\"Replacing infinity values with NaN and dropping corresponding rows...\")\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "initial_rows_X = X.shape[0]\n",
    "X.dropna(inplace=True)\n",
    "rows_dropped_X_na = initial_rows_X - X.shape[0]\n",
    "if rows_dropped_X_na > 0:\n",
    "    print(f\"Dropped {rows_dropped_X_na} rows from features (X) due to NA/infinity values.\")\n",
    "\n",
    "y = y[X.index]\n",
    "print(f\"Shape after dropping NAs/Infinities in features and aligning y: X={X.shape}, y={y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nMapping target labels to numerical values (Multiclass: BENIGN=0, others as unique classes)...\")\n",
    "y = y.astype(str).str.strip().str.upper()\n",
    "unique_labels_before_map = y.unique()\n",
    "print(f\"Unique labels before mapping: {unique_labels_before_map}\")\n",
    "label_mapping = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y = y.map(label_mapping)\n",
    "print(f\"Label mapping: {label_mapping}\")\n",
    "print(f\"Value counts for target after mapping:\\n{y.value_counts()}\")\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "if X.empty or X.shape[1] == 0:\n",
    "    raise ValueError(\"ERROR: No numeric features remaining after preprocessing.\")\n",
    "else:\n",
    "    print(f\"Final feature set (X) shape before split: {X.shape}\")\n",
    "\n",
    "# --- Step 5: Anomaly Detection Layer (Isolation Forest) ---\n",
    "print(f\"\\n--- Running Anomaly Detection with Isolation Forest ---\")\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "y_pred_anomaly = iso_forest.fit_predict(X)\n",
    "X_anomaly_filtered = X[y_pred_anomaly == 1]  # Keep only inliers\n",
    "y_anomaly_filtered = y[y_pred_anomaly == 1]\n",
    "print(f\"Removed {sum(y_pred_anomaly == -1)} outliers. Shape after anomaly filtering: X={X_anomaly_filtered.shape}, y={y_anomaly_filtered.shape}\")\n",
    "\n",
    "# --- Step 6: Feature Selection (SelectKBest) ---\n",
    "k_features = 20\n",
    "if X_anomaly_filtered.shape[1] > k_features:\n",
    "    print(f\"\\n--- Performing Feature Selection (SelectKBest with k={k_features}) ---\")\n",
    "    selector = SelectKBest(mutual_info_classif, k=k_features)\n",
    "    if X_anomaly_filtered.shape[0] == 0 or y_anomaly_filtered.shape[0] == 0:\n",
    "        print(\"WARNING: No data available for feature selection after cleaning. Skipping.\")\n",
    "        X_selected = X_anomaly_filtered.values\n",
    "    else:\n",
    "        X_selected = selector.fit_transform(X_anomaly_filtered, y_anomaly_filtered)\n",
    "        selected_feature_indices = selector.get_support(indices=True)\n",
    "        selected_feature_names = X_anomaly_filtered.columns[selected_feature_indices].tolist()\n",
    "        print(f\"Shape after feature selection: {X_selected.shape}\")\n",
    "        print(f\"Selected features: {selected_feature_names}\")\n",
    "else:\n",
    "    print(f\"\\n--- Skipping Feature Selection ---\")\n",
    "    print(f\"Number of available features ({X_anomaly_filtered.shape[1]}) is less than or equal to k_features ({k_features}). Using all numeric features.\")\n",
    "    X_selected = X_anomaly_filtered.values\n",
    "\n",
    "# --- Step 7: Train-Test Split ---\n",
    "print(f\"\\n--- Performing Train-Test Split ---\")\n",
    "if X_selected.shape[0] == 0 or X_selected.shape[1] == 0:\n",
    "    print(\"ERROR: No valid data available for splitting after all preprocessing steps.\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_selected, y_anomaly_filtered, test_size=0.2, random_state=42, stratify=y_anomaly_filtered\n",
    "    )\n",
    "    print(f\"Train-test split complete:\")\n",
    "    print(f\"  X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"  X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(f\"  y_train value counts:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"  y_test value counts:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "    # --- Step 8: Model Training and Evaluation (Multiclass Decision Tree with OneVsRest) ---\n",
    "    print(f\"\\n--- Training Multiclass Decision Tree with OneVsRest ---\")\n",
    "    clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    print(f\"\\n--- Evaluating Model Performance ---\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Ensure no NaN in predictions\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0)  # Replace NaN with 0 (default class)\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    # Ensure no NaN in probabilities\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.0)\n",
    "\n",
    "    print(\"--- Standard Metrics ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # Attack Type Analysis: Confusion Matrix Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix Heatmap by Attack Type')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report (Precision, Recall, F1-Score for each class):\")\n",
    "    print(classification_report(y_test, y_pred, target_names=list(label_mapping.keys())))\n",
    "\n",
    "    print(\"\\n--- Realistic Measures for Real-World Scenarios ---\")\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(f\"Precision (Weighted Average): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted Average): {recall:.4f}\")\n",
    "    print(f\"F1-Score (Weighted Average): {f1:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "\n",
    "    # ROC AUC for multiclass (one-vs-rest)\n",
    "    try:\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        y_test_bin = label_binarize(y_test, classes=range(len(label_mapping)))\n",
    "        # Ensure no NaN in y_test_bin or y_pred_proba\n",
    "        y_test_bin = np.nan_to_num(y_test_bin, nan=0)\n",
    "        y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.0)\n",
    "        roc_auc_ovr = roc_auc_score(y_test_bin, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        print(f\"ROC AUC Score (One-vs-Rest): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "        # Plot ROC Curve for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(len(label_mapping)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        for i, color in zip(range(len(label_mapping)), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve class {list(label_mapping.keys())[i]} (area = {roc_auc[i]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate (FPR)')\n",
    "        plt.ylabel('True Positive Rate (TPR) / Recall')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve (Multiclass)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC or plot ROC curve: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
